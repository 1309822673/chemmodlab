% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_train.R
\name{ModelTrain}
\alias{ModelTrain}
\title{Fit predictive models to sets of descriptors.}
\usage{
ModelTrain(data, ids = F, xcol.lengths = ifelse(ids, length(data) - 2,
  length(data) - 1), nfolds = 10, nsplits = 3, seed.in = NA,
  des.names = NA, models = c("NNet", "PLS", "LARs", "PLSLDA", "Tree", "SVM",
  "KNN", "Forest"), user.params = NULL)
}
\arguments{
\item{data}{a data frame containing an (optional) ID column,
a response column, and descriptor columns.  The columns should be
provide in this order.
The response variable should either be binary
(represented as a numeric vector with 0 or 1 values) or
continuous.  At the moment, only numeric descriptors are supported.}

\item{ids}{a logical.  Is an ID column provided?}

\item{xcol.lengths}{a vector of integers.  It is assumed that the columns
in \code{data} are grouped by descriptor set.  The integers specify the
number of descriptors in each descriptor set.  They should be ordered as
the descriptor sets are ordered in \code{data}.
Users can specify multiple descriptor sets. By default there is one
descriptor set, all columns in \code{data} except \code{ycol} and
\code{idcol}.}

\item{nfolds}{the number of folds to use for each cross
validation split.}

\item{nsplits}{the number of splits to use for repeated
cross validation.}

\item{seed.in}{a numeric vector with length equal to \code{nsplits}.
The seeds are used to randomly assign folds to observations for each
repeated cross-validation split. If \code{NA}, the first seed will be 
11111, the second will be 22222, and so on.}

\item{des.names}{a character vector specifying the names for each
descriptor
set.  The length of the vector must match the number of descriptor sets.
If \code{NA}, each descriptor set will be named "Descriptor Set i", where
i is the number of the descriptor set.}

\item{models}{a character vector specifying the regression or
classification models to use.  The strings must match models
implemented in `chemmodlab` (see Details).}

\item{user.params}{a list of data frames where each data frame contains
the parameters values for a model.  The list should have the format of
the list constructed by \code{\link{MakeModelDefaults}}. One can construct
a list of parameters using \code{\link{MakeModelDefaults}} and then
modify the parameters.}
}
\value{
A list is returned of class \code{\link{chemmodlab}} containing:
 \item{all.preds}{a list of lists of dataframes.  The elements of the outer
  list correspond to each CV split performed by \code{\link{ModelTrain}}. The
  elements of the inner list correspond to each descriptor set.  For each
  descriptor set and CV split combination, the output is a dataframe
  containing all model predictions.  The first column of each data frame
  contains the true value of the response.  The remaining columns contain
  the predictions for each model.}
\item{all.probs}{a list of lists of dataframes. Constructed only if there is
  a binary response.  The structure is the same as \code{all.preds}, except
  that predictions are replaced by "predicted probabilities" (ie. estimated
  probabilities of a response
  value of one).  Predicted
  probabilities are only reported for classification models.}
\item{model.acc}{a list of lists of model accuracy measures.  The elements of
  the outer list correspond each CV split performed by \code{ModelTrain}.
  The elements of the inner list correspond to each descriptor set.  For each
  descriptor set and CV split combination model accuracy measures for each model fit
  to the data.  Regression models are assessed with Pearson's \eqn{r} and
  \eqn{RMSE}. Classification models are assessed with contingency tables.}
\item{classify}{a logical.  Were classification models used for binary
  response?}
\item{responses}{a numeric vector.  The observed value of the response.}
\item{data}{a list of numeric matrices.  Each matrix is a descriptor set used
  as model input.}
\item{params}{a list of data frames as made by
  \code{\link{MakeModelDefaults}}.  Each data frame contains the parameters to
  be set for a particular model.}
}
\description{
\code{ModelTrain} fits a series of classification or regression
models to sets of descriptors and computes cross-validated measures
of model performance.
}
\details{
Multiple descriptor sets can be specified
by the user. For each descriptor set, repeated k-fold cross validation
is performed for the spcified number of regression and/or classification
models.

Not all modeling strategies will be appropriate for all response
types. For example, partial least squares linear discriminant analysis
("PLSLDA")
is not directly appropriate for continuous response assays such as
percent inhibition, but it can be applied once a threshold value for
percent inhibition is used to create a binary (active/inactive) response.

See \url{https://pages.github.ncsu.edu/jrash/chemmodlab/} for more 
information about the
models available (including model default parameters).

Sensible default values are selected for each
tunable model parameter, however users may set any parameter
manually using \code{\link{MakeModelDefaults}} and \code{user.params}.

\code{ModelTrain} predictions are based on k-fold cross-validation,
where the dataset is randomly divided into k parts, each containing
approximately equal numbers of compounds. Treating one of these parts
as a "test set" the remaining
k-1 parts are combined together as a "training set"
and used to build a model from the desired modeling technique and
descriptor set. This model is then applied to the "test set" to obtain
predictions. The process is repeated, holding out each of the k parts
in turn. One advantage of k-fold cross-validation is reduction in bias
from using the same data to both build and assess a model. Another
advantage is the increased precision of error estimation offered by
k-fold cross validation over a one-time split.

Recognizing that the definition of folds in k-fold cross validation
may have an impact on the observed performance measures, all models
are built using the same definition of folds. This process is repeated
to obtain multiple separate k-fold cross validation runs resulting in
multiple separate definitions of folds.  The number of these "splits"
is specified by \code{nsplits}.

Observed performance measures are
assessed across all splits using \code{\link{CombineSplits}}.  This
function assesses how sensitive performance measures are to fold
assignments, or changes to the training and test sets. 
Statistical tests are used to determine the best performing model and
descriptor set combination.
}
\examples{

# A data set with  binary response and multiple descriptor sets

desc_lengths <- c(24, 147)
desc_names <- c("BurdenNumbers", "Pharmacophores")
cml <- ModelTrain(aid364, ids = T, xcol.lengths = desc_lengths)
cml

# A continuous response

cml <- ModelTrain(USArrests)
cml

}
\author{
Jacqueline Hughes-Oliver, Jeremy Ash
}
\references{
?
}
\seealso{
\code{\link{chemmodlab}}, \code{\link{plot.chemmodlab}},
  \code{\link{CombineSplits}},
}

